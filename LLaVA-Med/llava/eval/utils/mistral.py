import time
from typing import Optional

from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import json
import torch
from tqdm import tqdm
from pathlib import Path
import re

MISTRAL_PROMPT_FILE_PATH = "/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/llava/eval/utils/prompts/mistral_prompt.txt"

def mistal_eval(model_output_file, mistral_eval_file:Optional[str] = None, batch_size:int = 16, closed=False, multilabel=False, data_categories=None):
    mistral_output_list=[]

    # --- Debug: Print Hugging Face cache directory ---
    from huggingface_hub.constants import HF_HUB_CACHE
    import os
    effective_cache_dir = os.environ.get("HF_HOME", HF_HUB_CACHE)
    print(f"[DEBUG] Effective Hugging Face Hub Cache Directory: {effective_cache_dir}")
    # ---

    # Load model and tokenizer from huggingface, forcing re-download
    print("[DEBUG] Attempting to load tokenizer with force_download=True and use_fast=False...") # Ensure this debug message reflects the change
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", force_download=True, use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token
    # load the model with float16 to fit the memory
    print("[DEBUG] Attempting to load model with force_download=True...")
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", device_map='cuda', torch_dtype=torch.bfloat16, force_download=True)

    # read the initial prompt
    if not closed:
        with open(MISTRAL_PROMPT_FILE_PATH,'r') as f:
            initial_prompt = f.read()
    elif closed and not multilabel:
        mistral_prompt_file_path = os.getenv("MISTRAL_PROMPT_FILE_PATH_CLOSED")
        with open(mistral_prompt_file_path,'r') as f:
            initial_prompt = f.read()
    else:
        mistral_prompt_file_path = os.getenv("MISTRAL_PROMPT_FILE_PATH_MULTILABEL")
        with open(mistral_prompt_file_path,'r') as f:
            initial_prompt = f.read()
    
    # Load the output JSON data generated by the model
    with open(model_output_file, 'r') as file:
        model_output_data = json.load(file)

    complete_input_list = []
    qids = []
    questions = []
    gts = []
    preds = []
    answer_types = []
    idx = 0

    for line in tqdm(model_output_data):
        if not closed:
            if line["answer_type"] == "closed" and idx != len(model_output_data) - 1:
                idx += 1
                continue
            if line["answer_type"] == "open":
                if line["text"] == line["ground_truth"]:
                    output_dict = {
                        "question_id": line["question_id"],
                        "prompt": line["prompt"],
                        "gt": line["ground_truth"],
                        "text": line["text"],
                        "answer_type": line["answer_type"],
                        "mistral_score": 5
                    }
                    mistral_output_list.append(output_dict)
                else:
                    raise ValueError("Should not run this part of code, review before running")
        else:
            raise ValueError("Should not run this part of code, review before running")
            if line["answer_type"] == "open" and idx != len(model_output_data) - 1:
                idx += 1
                continue
            if line["answer_type"] == "closed":
                if data_categories is not None:
                    categories = data_categories[data_categories['question_id'] == line["question_id"]]['list_categories'].iloc[0]
                else:
                    categories = None
                if categories is None:
                    if line["pred"] == line["gt"]:
                        output_dict = {
                            "qid": line["qid"],
                            "question": line["question"],
                            "gt": line["gt"],
                            "pred": line["pred"],
                            "answer_type": line["answer_type"],
                            "mistral_score": 1
                        }
                        mistral_output_list.append(output_dict)
                    else:
                        qids.append(line["qid"])
                        questions.append(line["question"])
                        gts.append(line["gt"])
                        preds.append(line["pred"])
                        answer_types.append(line["answer_type"])
                        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
                        complete_input_list.append(complete_input)
                elif len(categories) == 2 and not multilabel:
                    if line["pred"] == line["gt"]:
                        output_dict = {
                            "qid": line["qid"],
                            "question": line["question"],
                            "gt": line["gt"],
                            "pred": line["pred"],
                            "answer_type": line["answer_type"],
                            "mistral_score": 1
                        }
                        mistral_output_list.append(output_dict)
                    else:
                        qids.append(line["qid"])
                        questions.append(line["question"])
                        gts.append(line["gt"])
                        preds.append(line["pred"])
                        answer_types.append(line["answer_type"])
                        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
                        complete_input_list.append(complete_input)
                elif len(categories) > 2 and not multilabel:
                    idx += 1
                elif len(categories) > 2 and multilabel:
                    if line["pred"] == line["gt"]:
                        output_dict = {
                            "qid": line["qid"],
                            "question": line["question"],
                            "gt": line["gt"],
                            "pred": line["pred"],
                            "answer_type": line["answer_type"],
                            "mistral_score": 1
                        }
                        mistral_output_list.append(output_dict)
                    else:
                        qids.append(line["qid"])
                        questions.append(line["question"])
                        gts.append(line["gt"])
                        preds.append(line["pred"])
                        answer_types.append(line["answer_type"])
                        complete_input = initial_prompt + "[INST] " + str(line) + " [/INST]"
                        complete_input_list.append(complete_input)
                else:
                    idx += 1

        # print(len(complete_input_list))

        if len(complete_input_list) == batch_size or idx == len(model_output_data) - 1:
            if idx == len(model_output_data) - 1 and len(complete_input_list) == 0:
                continue
            input_ids = tokenizer(complete_input_list, return_tensors="pt", padding=True).input_ids.to("cuda")
            with torch.inference_mode():
                output_ids = model.generate(
                        input_ids,
                        max_new_tokens=200,
                    )
            input_token_len = input_ids.shape[1]
            outputs = tokenizer.batch_decode(
                output_ids[:, input_token_len:], skip_special_tokens=True
            )
            for i, output in enumerate(outputs):
                qid = qids[i]
                question = questions[i]
                gt = gts[i]
                pred = preds[i]
                answer_type = answer_types[i]
                output = output.strip()

                try:
                    # get the score value in the model output
                    score_obj = json.loads("{" + re.search("\"mistralscore\":(\s*)(\d*.?\d*)", output).group(0) + "}")
                    mistral_score = score_obj["mistralscore"]
                    # mistral_score = score_obj_float
                except ValueError:
                    # Handle the case where the pattern is not found in the outputs
                    # Where the mistral score is not numeric therefore it cannot be converted to float()
                    # Make the mistral score store the complete output so that it can be analyzed later
                    print("Retrieved mistral score object is being converted to float but it is not numeric")
                    print(f"Mistral score for the instance {qid} will be assigned to the complete text output of the Mistral model.")
                    print("Inspect this in the output JSON file")
                    mistral_score = output
                except Exception as e:
                    # Handle other potential errors
                    print(f"Mistral score for the instance {qid} will be assigned to the complete text output of the Mistral model.")
                    print("Inspect this in the output JSON file")
                    print("This is the original error message:\n", e)
                    mistral_score = output

                # create a dict from including the mdoel score
                output_dict={
                    "qid": qid,
                    "question": question,
                    "gt": gt,
                    "pred": pred,
                    "answer_type": answer_type,
                    "mistral_score": mistral_score
                }
                mistral_output_list.append(output_dict)
            complete_input_list = []
            qids = []
            questions = []
            gts = []
            preds = []
            answer_types = []
        idx += 1

    print("mistral_output_list", mistral_output_list)
    mistral_output_list = average_mistral_metrics(mistral_output_list, closed)
    if mistral_eval_file is not None:
        # save mistral evaluation as JSON
        if not Path(mistral_eval_file).parent.is_dir():
            os.makedirs(Path(mistral_eval_file).parent)
        with open(mistral_eval_file, 'w') as json_file:
            json.dump(mistral_output_list, json_file, indent=4)
    else:
        return mistral_output_list


def average_mistral_metrics(mistral_output_list, closed):
    sum_open_ended_score = 0
    num_open_qs=0
    sum_closed_ended_score = 0
    num_closed_qs=0
    for output in mistral_output_list:
        question_id = output["qid"]
        answer_type=output["answer_type"]
        mistral_score=output["mistral_score"]

        try:
            mistral_float_score = float(mistral_score)

            if not closed:
                if answer_type == "open":
                    sum_open_ended_score += mistral_float_score
                    num_open_qs += 1
                else:
                    continue
            else:
                if answer_type == "closed":
                    sum_closed_ended_score += mistral_float_score
                    num_closed_qs += 1
                else:
                    continue
        except ValueError:
            print(f"mistral score is not numeric for the question id {question_id}, this instance will be skipped during average mistral score calculation")
            print("If this is not desired, check the mistral metrics JSON file to fix the error")

    if not closed:
        average_scores = {
            "avg_mistral_score": sum_open_ended_score / max(1, num_open_qs)
        }
    else:
        average_scores = {
            "avg_mistral_score": sum_closed_ended_score / max(1, num_closed_qs)
        }
    mistral_output_list.insert(0, average_scores)

    return mistral_output_list


if __name__ == '__main__':
    start = time.perf_counter()
    model_output_file = "/nvme/VLMRobustness/Experiments/SLAKE/pretrained/eval/SLAKE_test_iid_modality_X-Ray/test_results.json"
    mistral_eval_file = f"/nvme/VLMRobustness/Experiments/SLAKE/pretrained/eval/SLAKE_test_iid_modality_X-Ray/mistral_scores.json"
    mistal_eval(model_output_file=model_output_file, mistral_eval_file=mistral_eval_file)
    end = time.perf_counter()
    time_sec = end - start
    time_min = time_sec / 60
    time_hours = time_min / 60
    with open(f'/nvme/VLMRobustness/Experiments/SLAKE/pretrained/eval/SLAKE_test_iid_modality_X-Ray/time.txt', 'w') as f:
        f.write(f'Eval took {time_sec:0.4f} seconds, {time_min:0.4f} minutes, {time_hours:0.4f} hours\n')
    # config = get_config()
    # mistal_eval(config)
    # average_mistral_metrics(config)