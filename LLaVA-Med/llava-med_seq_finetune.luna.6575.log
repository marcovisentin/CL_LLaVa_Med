--run_all_binary_sequences is active. Preparing 12 binary sequences.
Correctly generated 12 binary sequences for the 4 available datasets.


============================================================
Processing Sequence 1/12: binary_CT_to_Pathology
Output Base Directory: /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology
============================================================


########################################
Sequence: binary_CT_to_Pathology | Stage 1/2: Processing Dataset: CT
Input model for this stage: llava-med-v1.5-mistral-7b
########################################
Starting training for sequence 'binary_CT_to_Pathology', stage: CT on dataset CT
Output directory for this stage: /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT

==================== Executing TRAINING ====================
Model: Seq: binary_CT_to_Pathology, Stage Input: llava-med-v1.5-mistral-7b
Dataset: CT
Command: deepspeed --master_port=29737 /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path microsoft/llava-med-v1.5-mistral-7b --data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/train/annotations.json --image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/train --output_dir /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT --eval_data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/val/annotations.json --eval_image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/val --wandb_project llava-med-binary-sequential-training_binary_CT_to_Pathology --version mistral_instruct --tune_mm_mlp_adapter True --bf16 True --num_train_epochs 2 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --learning_rate 1e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --save_strategy steps --save_total_limit 1 --evaluation_strategy steps --load_best_model_at_end True --metric_for_best_model eval_loss --greater_is_better False --prediction_loss_only True --eval_from_train_set False --evaluation_strategy steps --eval_steps 50 --eval_accumulation_steps 1

--- STDOUT (TRAINING on CT) ---
[2025-05-30 13:51:24,608] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 13:52:09,622] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0
[2025-05-30 13:52:09,622] [INFO] [runner.py:571:main] cmd = /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29737 --enable_each_rank_log=None /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path microsoft/llava-med-v1.5-mistral-7b --data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/train/annotations.json --image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/train --output_dir /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT --eval_data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/val/annotations.json --eval_image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/val --wandb_project llava-med-binary-sequential-training_binary_CT_to_Pathology --version mistral_instruct --tune_mm_mlp_adapter True --bf16 True --num_train_epochs 2 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --learning_rate 1e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --save_strategy steps --save_total_limit 1 --evaluation_strategy steps --load_best_model_at_end True --metric_for_best_model eval_loss --greater_is_better False --prediction_loss_only True --eval_from_train_set False --evaluation_strategy steps --eval_steps 50 --eval_accumulation_steps 1
[2025-05-30 13:52:12,891] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 13:52:15,375] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-05-30 13:52:15,375] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-05-30 13:52:15,375] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-05-30 13:52:15,375] [INFO] [launch.py:163:main] dist_world_size=1
[2025-05-30 13:52:15,375] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-05-30 13:52:20,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 13:52:59,792] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-05-30 13:52:59,793] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
WANDB login...
Loading model...
Enabling gradient checkpointing...
Setting conversation template as Conversation(system='', roles=('USER', 'ASSISTANT'), messages=(), offset=0, sep_style=<SeparatorStyle.LLAMA_2: 5>, sep='', sep2='</s>', version='llama_v2', skip_next=False)
Tuning mm projector...
Model training parameters:
model.mm_projector.0.weight: torch.Size([4096, 1024])
model.mm_projector.0.bias: torch.Size([4096])
model.mm_projector.2.weight: torch.Size([4096, 4096])
model.mm_projector.2.bias: torch.Size([4096])
Making supervised data module...
Formatting inputs...Skip in lazy mode
Loaded 3653 samples from /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/train/annotations.json
Loading eval dataset...
Formatting inputs...Skip in lazy mode
Loaded 521 samples from /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/CT/val/annotations.json
{'loss': 4.108, 'learning_rate': 7.142857142857143e-05, 'epoch': 0.04}
{'loss': 1.7828, 'learning_rate': 9.995494831023409e-05, 'epoch': 0.09}
{'loss': 1.5012, 'learning_rate': 9.967992638098515e-05, 'epoch': 0.13}
{'loss': 1.1526, 'learning_rate': 9.915628588978522e-05, 'epoch': 0.17}
{'loss': 1.1036, 'learning_rate': 9.838664734667495e-05, 'epoch': 0.22}
{'eval_loss': 0.9569380283355713, 'eval_runtime': 37.2784, 'eval_samples_per_second': 13.976, 'eval_steps_per_second': 0.885, 'epoch': 0.22}
{'loss': 0.9726, 'learning_rate': 9.737486233602148e-05, 'epoch': 0.26}
{'loss': 0.861, 'learning_rate': 9.612599424162344e-05, 'epoch': 0.31}
{'loss': 1.0348, 'learning_rate': 9.464629290747842e-05, 'epoch': 0.35}
{'loss': 0.9497, 'learning_rate': 9.294316336102132e-05, 'epoch': 0.39}
{'loss': 0.8224, 'learning_rate': 9.102512875535438e-05, 'epoch': 0.44}
{'eval_loss': 0.7594570517539978, 'eval_runtime': 36.6146, 'eval_samples_per_second': 14.229, 'eval_steps_per_second': 0.901, 'epoch': 0.44}
{'loss': 0.8545, 'learning_rate': 8.890178771592199e-05, 'epoch': 0.48}
{'loss': 0.8967, 'learning_rate': 8.658376630508392e-05, 'epoch': 0.52}
{'loss': 0.8856, 'learning_rate': 8.408266484497664e-05, 'epoch': 0.57}
{'loss': 0.8452, 'learning_rate': 8.141099986478212e-05, 'epoch': 0.61}
{'loss': 0.82, 'learning_rate': 7.858214146292394e-05, 'epoch': 0.66}
{'eval_loss': 0.6844056248664856, 'eval_runtime': 36.545, 'eval_samples_per_second': 14.256, 'eval_steps_per_second': 0.903, 'epoch': 0.66}
{'loss': 0.7302, 'learning_rate': 7.561024639765571e-05, 'epoch': 0.7}
{'loss': 0.7234, 'learning_rate': 7.251018724088367e-05, 'epoch': 0.74}
{'loss': 0.7397, 'learning_rate': 6.929747794976645e-05, 'epoch': 0.79}
{'loss': 0.7137, 'learning_rate': 6.598819622856227e-05, 'epoch': 0.83}
{'loss': 0.7366, 'learning_rate': 6.259890306925627e-05, 'epoch': 0.87}
{'eval_loss': 0.6221122741699219, 'eval_runtime': 36.2385, 'eval_samples_per_second': 14.377, 'eval_steps_per_second': 0.911, 'epoch': 0.87}
{'loss': 0.7778, 'learning_rate': 5.9146559873619335e-05, 'epoch': 0.92}
{'loss': 0.7176, 'learning_rate': 5.564844357145364e-05, 'epoch': 0.96}
{'loss': 0.8729, 'learning_rate': 5.212206015980742e-05, 'epoch': 1.0}
{'loss': 0.6312, 'learning_rate': 4.85850570958441e-05, 'epoch': 1.05}
{'loss': 0.67, 'learning_rate': 4.5055134981787515e-05, 'epoch': 1.09}
{'eval_loss': 0.6033448576927185, 'eval_runtime': 36.214, 'eval_samples_per_second': 14.387, 'eval_steps_per_second': 0.911, 'epoch': 1.09}
{'loss': 0.6356, 'learning_rate': 4.1549958983907555e-05, 'epoch': 1.14}
{'loss': 0.7548, 'learning_rate': 3.808707042884176e-05, 'epoch': 1.18}
{'loss': 0.4799, 'learning_rate': 3.468379901966083e-05, 'epoch': 1.22}
{'loss': 0.5148, 'learning_rate': 3.135717611098458e-05, 'epoch': 1.27}
{'loss': 0.5815, 'learning_rate': 2.8123849477154805e-05, 'epoch': 1.31}
{'eval_loss': 0.59098219871521, 'eval_runtime': 36.1688, 'eval_samples_per_second': 14.405, 'eval_steps_per_second': 0.912, 'epoch': 1.31}
{'loss': 0.5707, 'learning_rate': 2.500000000000001e-05, 'epoch': 1.35}
{'loss': 0.5795, 'learning_rate': 2.2001260693120233e-05, 'epoch': 1.4}
{'loss': 0.4884, 'learning_rate': 1.9142638467927254e-05, 'epoch': 1.44}
{'loss': 0.6941, 'learning_rate': 1.6438439032954855e-05, 'epoch': 1.48}
{'loss': 0.6501, 'learning_rate': 1.3902195302273779e-05, 'epoch': 1.53}
{'eval_loss': 0.5679824948310852, 'eval_runtime': 36.0433, 'eval_samples_per_second': 14.455, 'eval_steps_per_second': 0.916, 'epoch': 1.53}
{'loss': 0.5134, 'learning_rate': 1.1546599671284159e-05, 'epoch': 1.57}
{'loss': 0.711, 'learning_rate': 9.383440498805712e-06, 'epoch': 1.62}
{'loss': 0.5917, 'learning_rate': 7.423543113334436e-06, 'epoch': 1.66}
{'loss': 0.6265, 'learning_rate': 5.676715638695063e-06, 'epoch': 1.7}
{'loss': 0.5291, 'learning_rate': 4.151699910199336e-06, 'epoch': 1.75}
{'eval_loss': 0.5602948665618896, 'eval_runtime': 36.13, 'eval_samples_per_second': 14.42, 'eval_steps_per_second': 0.913, 'epoch': 1.75}
{'loss': 0.6184, 'learning_rate': 2.85612772694579e-06, 'epoch': 1.79}
{'loss': 0.6383, 'learning_rate': 1.796482659192472e-06, 'epoch': 1.83}
{'loss': 0.6383, 'learning_rate': 9.780676019336631e-07, 'epoch': 1.88}
{'loss': 0.6055, 'learning_rate': 4.049782370561583e-07, 'epoch': 1.92}
{'loss': 0.6561, 'learning_rate': 8.008253688084889e-08, 'epoch': 1.97}
{'eval_loss': 0.5580126643180847, 'eval_runtime': 36.2478, 'eval_samples_per_second': 14.373, 'eval_steps_per_second': 0.91, 'epoch': 1.97}
{'train_runtime': 1924.2548, 'train_samples_per_second': 3.797, 'train_steps_per_second': 0.238, 'train_loss': 0.8400613626538406, 'epoch': 2.0}
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mtrained_on_CT[0m at: [34mhttps://wandb.ai/mv320/llava-med-binary-sequential-training_binary_CT_to_Pathology/runs/l64r979s[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250530_135300-l64r979s/logs[0m
[2025-05-30 14:29:42,778] [INFO] [launch.py:347:main] Process 3916172 exits successfully.

--- STDERR (TRAINING on CT) ---
wandb: Currently logged in as: mv320. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.4
wandb: Run data is saved locally in /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/wandb/run-20250530_135300-l64r979s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trained_on_CT
wandb: ⭐️ View project at https://wandb.ai/mv320/llava-med-binary-sequential-training_binary_CT_to_Pathology
wandb: 🚀 View run at https://wandb.ai/mv320/llava-med-binary-sequential-training_binary_CT_to_Pathology/runs/l64r979s
/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:50<02:31, 50.58s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [01:42<01:42, 51.17s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [02:30<00:49, 49.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:32<00:00, 30.96s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:32<00:00, 38.08s/it]
Some weights of the model checkpoint at microsoft/llava-med-v1.5-mistral-7b were not used when initializing LlavaMistralForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaMistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaMistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()

  0%|          | 0/458 [00:00<?, ?it/s]/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])

  0%|          | 1/458 [00:20<2:39:44, 20.97s/it]
  0%|          | 2/458 [00:24<1:20:26, 10.59s/it]
  1%|          | 3/458 [00:27<54:58,  7.25s/it]  
  1%|          | 4/458 [00:30<43:00,  5.68s/it]
  1%|          | 5/458 [00:34<36:32,  4.84s/it]
  1%|▏         | 6/458 [00:37<32:47,  4.35s/it]
  2%|▏         | 7/458 [00:41<30:24,  4.05s/it]
  2%|▏         | 8/458 [00:44<28:41,  3.83s/it]
  2%|▏         | 9/458 [00:47<27:39,  3.70s/it]
  2%|▏         | 10/458 [00:51<26:56,  3.61s/it]
                                                

  2%|▏         | 10/458 [00:51<26:56,  3.61s/it]
  2%|▏         | 11/458 [00:54<26:42,  3.58s/it]
  3%|▎         | 12/458 [00:58<26:17,  3.54s/it]
  3%|▎         | 13/458 [01:01<25:58,  3.50s/it]
  3%|▎         | 14/458 [01:05<25:46,  3.48s/it]
  3%|▎         | 15/458 [01:08<25:24,  3.44s/it]
  3%|▎         | 16/458 [01:11<25:21,  3.44s/it]
  4%|▎         | 17/458 [01:15<25:18,  3.44s/it]
  4%|▍         | 18/458 [01:18<25:15,  3.44s/it]
  4%|▍         | 19/458 [01:22<25:13,  3.45s/it]
  4%|▍         | 20/458 [01:25<25:07,  3.44s/it]
                                                

  4%|▍         | 20/458 [01:25<25:07,  3.44s/it]
  5%|▍         | 21/458 [01:29<25:05,  3.45s/it]
  5%|▍         | 22/458 [01:32<24:59,  3.44s/it]
  5%|▌         | 23/458 [01:35<24:59,  3.45s/it]
  5%|▌         | 24/458 [01:39<24:55,  3.45s/it]
  5%|▌         | 25/458 [01:42<24:52,  3.45s/it]
  6%|▌         | 26/458 [01:46<24:43,  3.43s/it]
  6%|▌         | 27/458 [01:49<24:29,  3.41s/it]
  6%|▌         | 28/458 [01:53<24:30,  3.42s/it]
  6%|▋         | 29/458 [01:56<24:23,  3.41s/it]
  7%|▋         | 30/458 [01:59<24:24,  3.42s/it]
                                                

  7%|▋         | 30/458 [01:59<24:24,  3.42s/it]
  7%|▋         | 31/458 [02:03<24:10,  3.40s/it]
  7%|▋         | 32/458 [02:06<24:13,  3.41s/it]
  7%|▋         | 33/458 [02:10<24:07,  3.41s/it]
  7%|▋         | 34/458 [02:13<23:55,  3.39s/it]
  8%|▊         | 35/458 [02:16<24:00,  3.40s/it]
  8%|▊         | 36/458 [02:20<24:03,  3.42s/it]
  8%|▊         | 37/458 [02:23<23:51,  3.40s/it]
  8%|▊         | 38/458 [02:27<23:55,  3.42s/it]
  9%|▊         | 39/458 [02:30<23:56,  3.43s/it]
  9%|▊         | 40/458 [02:33<23:56,  3.44s/it]
                                                

  9%|▊         | 40/458 [02:33<23:56,  3.44s/it]
  9%|▉         | 41/458 [02:37<23:56,  3.45s/it]
  9%|▉         | 42/458 [02:40<23:56,  3.45s/it]
  9%|▉         | 43/458 [02:44<23:55,  3.46s/it]
 10%|▉         | 44/458 [02:47<23:39,  3.43s/it]
 10%|▉         | 45/458 [02:51<23:27,  3.41s/it]
 10%|█         | 46/458 [02:54<23:27,  3.42s/it]
 10%|█         | 47/458 [02:58<23:28,  3.43s/it]
 10%|█         | 48/458 [03:01<23:28,  3.43s/it]
 11%|█         | 49/458 [03:04<23:15,  3.41s/it]
 11%|█         | 50/458 [03:08<23:18,  3.43s/it]
                                                

 11%|█         | 50/458 [03:08<23:18,  3.43s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:17,  1.78it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.29it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.12it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:26,  1.00it/s][A

 21%|██        | 7/33 [00:06<00:26,  1.02s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.04s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.05s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.06s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.07s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.07s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.08s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.09s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.10s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.09s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.08s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.08s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.08s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.03s/it][A
                                                

                                               
[A
 11%|█         | 50/458 [03:45<23:18,  3.43s/it]

100%|██████████| 33/33 [00:34<00:00,  1.03s/it][A

                                               [A
 11%|█         | 51/458 [03:49<1:39:14, 14.63s/it]
 11%|█▏        | 52/458 [03:52<1:16:22, 11.29s/it]
 12%|█▏        | 53/458 [03:56<1:00:22,  8.95s/it]
 12%|█▏        | 54/458 [03:59<49:12,  7.31s/it]  
 12%|█▏        | 55/458 [04:02<41:20,  6.15s/it]
 12%|█▏        | 56/458 [04:06<35:40,  5.32s/it]
 12%|█▏        | 57/458 [04:09<31:52,  4.77s/it]
 13%|█▎        | 58/458 [04:13<29:01,  4.35s/it]
 13%|█▎        | 59/458 [04:16<27:14,  4.10s/it]
 13%|█▎        | 60/458 [04:20<25:57,  3.91s/it]
                                                

 13%|█▎        | 60/458 [04:20<25:57,  3.91s/it]
 13%|█▎        | 61/458 [04:23<25:02,  3.78s/it]
 14%|█▎        | 62/458 [04:27<24:11,  3.66s/it]
 14%|█▍        | 63/458 [04:30<23:45,  3.61s/it]
 14%|█▍        | 64/458 [04:33<23:22,  3.56s/it]
 14%|█▍        | 65/458 [04:37<23:10,  3.54s/it]
 14%|█▍        | 66/458 [04:40<22:48,  3.49s/it]
 15%|█▍        | 67/458 [04:44<22:44,  3.49s/it]
 15%|█▍        | 68/458 [04:47<22:40,  3.49s/it]
 15%|█▌        | 69/458 [04:51<22:35,  3.49s/it]
 15%|█▌        | 70/458 [04:54<22:25,  3.47s/it]
                                                

 15%|█▌        | 70/458 [04:54<22:25,  3.47s/it]
 16%|█▌        | 71/458 [04:58<22:11,  3.44s/it]
 16%|█▌        | 72/458 [05:01<22:12,  3.45s/it]
 16%|█▌        | 73/458 [05:05<22:12,  3.46s/it]
 16%|█▌        | 74/458 [05:08<22:00,  3.44s/it]
 16%|█▋        | 75/458 [05:11<21:57,  3.44s/it]
 17%|█▋        | 76/458 [05:15<21:59,  3.45s/it]
 17%|█▋        | 77/458 [05:18<21:57,  3.46s/it]
 17%|█▋        | 78/458 [05:22<21:55,  3.46s/it]
 17%|█▋        | 79/458 [05:25<21:51,  3.46s/it]
 17%|█▋        | 80/458 [05:29<21:44,  3.45s/it]
                                                

 17%|█▋        | 80/458 [05:29<21:44,  3.45s/it]
 18%|█▊        | 81/458 [05:32<21:44,  3.46s/it]
 18%|█▊        | 82/458 [05:36<21:38,  3.45s/it]
 18%|█▊        | 83/458 [05:39<21:26,  3.43s/it]
 18%|█▊        | 84/458 [05:42<21:29,  3.45s/it]
 19%|█▊        | 85/458 [05:46<21:18,  3.43s/it]
 19%|█▉        | 86/458 [05:49<21:20,  3.44s/it]
 19%|█▉        | 87/458 [05:53<21:21,  3.45s/it]
 19%|█▉        | 88/458 [05:56<21:19,  3.46s/it]
 19%|█▉        | 89/458 [06:00<21:12,  3.45s/it]
 20%|█▉        | 90/458 [06:03<21:10,  3.45s/it]
                                                

 20%|█▉        | 90/458 [06:03<21:10,  3.45s/it]
 20%|█▉        | 91/458 [06:07<21:04,  3.45s/it]
 20%|██        | 92/458 [06:10<20:59,  3.44s/it]
 20%|██        | 93/458 [06:13<20:47,  3.42s/it]
 21%|██        | 94/458 [06:17<20:49,  3.43s/it]
 21%|██        | 95/458 [06:20<20:49,  3.44s/it]
 21%|██        | 96/458 [06:24<20:37,  3.42s/it]
 21%|██        | 97/458 [06:27<20:40,  3.44s/it]
 21%|██▏       | 98/458 [06:31<20:37,  3.44s/it]
 22%|██▏       | 99/458 [06:34<20:32,  3.43s/it]
 22%|██▏       | 100/458 [06:38<20:33,  3.45s/it]
                                                 

 22%|██▏       | 100/458 [06:38<20:33,  3.45s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.85it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.13it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.05it/s][A

 18%|█▊        | 6/33 [00:05<00:26,  1.00it/s][A

 21%|██        | 7/33 [00:06<00:26,  1.02s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.04s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.07s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.08s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.08s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:24<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.09s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.09s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.03s/it][A
                                                 

                                               
[A
 22%|██▏       | 100/458 [07:14<20:33,  3.45s/it]

100%|██████████| 33/33 [00:34<00:00,  1.03s/it][A

                                               [A
 22%|██▏       | 101/458 [07:18<1:25:56, 14.44s/it]
 22%|██▏       | 102/458 [07:21<1:06:06, 11.14s/it]
 22%|██▏       | 103/458 [07:25<52:13,  8.83s/it]  
 23%|██▎       | 104/458 [07:28<42:24,  7.19s/it]
 23%|██▎       | 105/458 [07:31<35:44,  6.07s/it]
 23%|██▎       | 106/458 [07:35<30:59,  5.28s/it]
 23%|██▎       | 107/458 [07:38<27:44,  4.74s/it]
 24%|██▎       | 108/458 [07:42<25:28,  4.37s/it]
 24%|██▍       | 109/458 [07:45<23:39,  4.07s/it]
 24%|██▍       | 110/458 [07:49<22:33,  3.89s/it]
                                                 

 24%|██▍       | 110/458 [07:49<22:33,  3.89s/it]
 24%|██▍       | 111/458 [07:52<21:46,  3.77s/it]
 24%|██▍       | 112/458 [07:56<21:09,  3.67s/it]
 25%|██▍       | 113/458 [07:59<20:34,  3.58s/it]
 25%|██▍       | 114/458 [08:02<20:20,  3.55s/it]
 25%|██▌       | 115/458 [08:06<20:09,  3.53s/it]
 25%|██▌       | 116/458 [08:09<19:59,  3.51s/it]
 26%|██▌       | 117/458 [08:13<19:48,  3.48s/it]
 26%|██▌       | 118/458 [08:16<19:44,  3.48s/it]
 26%|██▌       | 119/458 [08:20<19:34,  3.46s/it]
 26%|██▌       | 120/458 [08:23<19:28,  3.46s/it]
                                                 

 26%|██▌       | 120/458 [08:23<19:28,  3.46s/it]
 26%|██▋       | 121/458 [08:27<19:36,  3.49s/it]
 27%|██▋       | 122/458 [08:30<19:30,  3.48s/it]
 27%|██▋       | 123/458 [08:34<19:22,  3.47s/it]
 27%|██▋       | 124/458 [08:37<19:20,  3.47s/it]
 27%|██▋       | 125/458 [08:40<19:07,  3.45s/it]
 28%|██▊       | 126/458 [08:44<19:08,  3.46s/it]
 28%|██▊       | 127/458 [08:47<19:07,  3.47s/it]
 28%|██▊       | 128/458 [08:51<19:06,  3.47s/it]
 28%|██▊       | 129/458 [08:54<19:04,  3.48s/it]
 28%|██▊       | 130/458 [08:58<19:02,  3.48s/it]
                                                 

 28%|██▊       | 130/458 [08:58<19:02,  3.48s/it]
 29%|██▊       | 131/458 [09:01<18:55,  3.47s/it]
 29%|██▉       | 132/458 [09:05<18:43,  3.45s/it]
 29%|██▉       | 133/458 [09:08<18:44,  3.46s/it]
 29%|██▉       | 134/458 [09:12<18:33,  3.44s/it]
 29%|██▉       | 135/458 [09:15<18:35,  3.45s/it]
 30%|██▉       | 136/458 [09:19<18:34,  3.46s/it]
 30%|██▉       | 137/458 [09:22<18:33,  3.47s/it]
 30%|███       | 138/458 [09:25<18:30,  3.47s/it]
 30%|███       | 139/458 [09:29<18:23,  3.46s/it]
 31%|███       | 140/458 [09:32<18:22,  3.47s/it]
                                                 

 31%|███       | 140/458 [09:32<18:22,  3.47s/it]
 31%|███       | 141/458 [09:36<18:15,  3.45s/it]
 31%|███       | 142/458 [09:39<18:14,  3.47s/it]
 31%|███       | 143/458 [09:43<18:13,  3.47s/it]
 31%|███▏      | 144/458 [09:46<18:10,  3.47s/it]
 32%|███▏      | 145/458 [09:50<18:03,  3.46s/it]
 32%|███▏      | 146/458 [09:53<17:57,  3.45s/it]
 32%|███▏      | 147/458 [09:57<17:56,  3.46s/it]
 32%|███▏      | 148/458 [10:00<17:55,  3.47s/it]
 33%|███▎      | 149/458 [10:04<17:53,  3.47s/it]
 33%|███▎      | 150/458 [10:07<17:48,  3.47s/it]
                                                 

 33%|███▎      | 150/458 [10:07<17:48,  3.47s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.84it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.13it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:26,  1.00it/s][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.07s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.08s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.09s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.08s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.08s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.02s/it][A
                                                 

                                               
[A
 33%|███▎      | 150/458 [10:44<17:48,  3.47s/it]

100%|██████████| 33/33 [00:34<00:00,  1.02s/it][A

                                               [A
 33%|███▎      | 151/458 [10:47<1:13:43, 14.41s/it]
 33%|███▎      | 152/458 [10:50<56:36, 11.10s/it]  
 33%|███▎      | 153/458 [10:54<44:49,  8.82s/it]
 34%|███▎      | 154/458 [10:57<36:29,  7.20s/it]
 34%|███▍      | 155/458 [11:01<30:35,  6.06s/it]
 34%|███▍      | 156/458 [11:04<26:36,  5.29s/it]
 34%|███▍      | 157/458 [11:08<23:44,  4.73s/it]
 34%|███▍      | 158/458 [11:11<21:48,  4.36s/it]
 35%|███▍      | 159/458 [11:15<20:22,  4.09s/it]
 35%|███▍      | 160/458 [11:18<19:16,  3.88s/it]
                                                 

 35%|███▍      | 160/458 [11:18<19:16,  3.88s/it]
 35%|███▌      | 161/458 [11:21<18:29,  3.74s/it]
 35%|███▌      | 162/458 [11:25<18:04,  3.66s/it]
 36%|███▌      | 163/458 [11:28<17:45,  3.61s/it]
 36%|███▌      | 164/458 [11:32<17:31,  3.58s/it]
 36%|███▌      | 165/458 [11:35<17:20,  3.55s/it]
 36%|███▌      | 166/458 [11:39<17:02,  3.50s/it]
 36%|███▋      | 167/458 [11:42<16:57,  3.50s/it]
 37%|███▋      | 168/458 [11:46<16:45,  3.47s/it]
 37%|███▋      | 169/458 [11:49<16:43,  3.47s/it]
 37%|███▋      | 170/458 [11:52<16:33,  3.45s/it]
                                                 

 37%|███▋      | 170/458 [11:52<16:33,  3.45s/it]
 37%|███▋      | 171/458 [11:56<16:34,  3.46s/it]
 38%|███▊      | 172/458 [11:59<16:24,  3.44s/it]
 38%|███▊      | 173/458 [12:03<16:38,  3.50s/it]
 38%|███▊      | 174/458 [12:07<16:40,  3.52s/it]
 38%|███▊      | 175/458 [12:10<16:34,  3.51s/it]
 38%|███▊      | 176/458 [12:13<16:20,  3.48s/it]
 39%|███▊      | 177/458 [12:17<16:17,  3.48s/it]
 39%|███▉      | 178/458 [12:20<16:07,  3.46s/it]
 39%|███▉      | 179/458 [12:24<16:07,  3.47s/it]
 39%|███▉      | 180/458 [12:27<16:05,  3.47s/it]
                                                 

 39%|███▉      | 180/458 [12:27<16:05,  3.47s/it]
 40%|███▉      | 181/458 [12:31<16:03,  3.48s/it]
 40%|███▉      | 182/458 [12:34<16:01,  3.48s/it]
 40%|███▉      | 183/458 [12:38<15:59,  3.49s/it]
 40%|████      | 184/458 [12:41<15:49,  3.46s/it]
 40%|████      | 185/458 [12:45<15:48,  3.47s/it]
 41%|████      | 186/458 [12:48<15:46,  3.48s/it]
 41%|████      | 187/458 [12:52<15:40,  3.47s/it]
 41%|████      | 188/458 [12:55<15:31,  3.45s/it]
 41%|████▏     | 189/458 [12:59<15:29,  3.45s/it]
 41%|████▏     | 190/458 [13:02<15:30,  3.47s/it]
                                                 

 41%|████▏     | 190/458 [13:02<15:30,  3.47s/it]
 42%|████▏     | 191/458 [13:06<15:29,  3.48s/it]
 42%|████▏     | 192/458 [13:09<15:26,  3.48s/it]
 42%|████▏     | 193/458 [13:13<15:23,  3.49s/it]
 42%|████▏     | 194/458 [13:16<15:20,  3.49s/it]
 43%|████▎     | 195/458 [13:20<15:18,  3.49s/it]
 43%|████▎     | 196/458 [13:23<15:15,  3.49s/it]
 43%|████▎     | 197/458 [13:27<15:12,  3.50s/it]
 43%|████▎     | 198/458 [13:30<15:08,  3.49s/it]
 43%|████▎     | 199/458 [13:33<15:02,  3.48s/it]
 44%|████▎     | 200/458 [13:37<15:00,  3.49s/it]
                                                 

 44%|████▎     | 200/458 [13:37<15:00,  3.49s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.84it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.12it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:27,  1.00s/it][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.08s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.09s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.09s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.09s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.09s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.09s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.09s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.09s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A
                                                 

                                               
[A
 44%|████▎     | 200/458 [14:13<15:00,  3.49s/it]

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A

                                               [A
 44%|████▍     | 201/458 [14:17<1:01:28, 14.35s/it]
 44%|████▍     | 202/458 [14:20<47:20, 11.09s/it]  
 44%|████▍     | 203/458 [14:24<37:27,  8.81s/it]
 45%|████▍     | 204/458 [14:27<30:33,  7.22s/it]
 45%|████▍     | 205/458 [14:31<25:39,  6.09s/it]
 45%|████▍     | 206/458 [14:34<22:09,  5.28s/it]
 45%|████▌     | 207/458 [14:37<19:49,  4.74s/it]
 45%|████▌     | 208/458 [14:41<18:07,  4.35s/it]
 46%|████▌     | 209/458 [14:44<16:55,  4.08s/it]
 46%|████▌     | 210/458 [14:48<16:07,  3.90s/it]
                                                 

 46%|████▌     | 210/458 [14:48<16:07,  3.90s/it]
 46%|████▌     | 211/458 [14:51<15:34,  3.78s/it]
 46%|████▋     | 212/458 [14:55<15:09,  3.70s/it]
 47%|████▋     | 213/458 [14:58<14:50,  3.63s/it]
 47%|████▋     | 214/458 [15:02<14:29,  3.56s/it]
 47%|████▋     | 215/458 [15:05<14:13,  3.51s/it]
 47%|████▋     | 216/458 [15:09<14:08,  3.51s/it]
 47%|████▋     | 217/458 [15:12<14:01,  3.49s/it]
 48%|████▊     | 218/458 [15:16<13:57,  3.49s/it]
 48%|████▊     | 219/458 [15:19<13:53,  3.49s/it]
 48%|████▊     | 220/458 [15:22<13:42,  3.46s/it]
                                                 

 48%|████▊     | 220/458 [15:22<13:42,  3.46s/it]
 48%|████▊     | 221/458 [15:26<13:37,  3.45s/it]
 48%|████▊     | 222/458 [15:29<13:37,  3.46s/it]
 49%|████▊     | 223/458 [15:33<13:35,  3.47s/it]
 49%|████▉     | 224/458 [15:36<13:33,  3.48s/it]
 49%|████▉     | 225/458 [15:40<13:24,  3.45s/it]
 49%|████▉     | 226/458 [15:43<13:23,  3.46s/it]
 50%|████▉     | 227/458 [15:47<13:22,  3.47s/it]
 50%|████▉     | 228/458 [15:50<13:16,  3.47s/it]
 50%|█████     | 229/458 [15:52<10:53,  2.85s/it]
 50%|█████     | 230/458 [15:56<12:22,  3.25s/it]
                                                 

 50%|█████     | 230/458 [15:56<12:22,  3.25s/it]
 50%|█████     | 231/458 [15:59<12:28,  3.30s/it]
 51%|█████     | 232/458 [16:03<12:36,  3.35s/it]
 51%|█████     | 233/458 [16:06<12:43,  3.39s/it]
 51%|█████     | 234/458 [16:10<12:46,  3.42s/it]
 51%|█████▏    | 235/458 [16:13<12:44,  3.43s/it]
 52%|█████▏    | 236/458 [16:17<12:46,  3.45s/it]
 52%|█████▏    | 237/458 [16:20<12:45,  3.46s/it]
 52%|█████▏    | 238/458 [16:23<12:37,  3.44s/it]
 52%|█████▏    | 239/458 [16:27<12:36,  3.46s/it]
 52%|█████▏    | 240/458 [16:30<12:32,  3.45s/it]
                                                 

 52%|█████▏    | 240/458 [16:30<12:32,  3.45s/it]
 53%|█████▎    | 241/458 [16:34<12:24,  3.43s/it]
 53%|█████▎    | 242/458 [16:37<12:17,  3.41s/it]
 53%|█████▎    | 243/458 [16:41<12:16,  3.43s/it]
 53%|█████▎    | 244/458 [16:44<12:16,  3.44s/it]
 53%|█████▎    | 245/458 [16:48<12:16,  3.46s/it]
 54%|█████▎    | 246/458 [16:51<12:14,  3.46s/it]
 54%|█████▍    | 247/458 [16:54<12:05,  3.44s/it]
 54%|█████▍    | 248/458 [16:58<12:04,  3.45s/it]
 54%|█████▍    | 249/458 [17:01<12:04,  3.46s/it]
 55%|█████▍    | 250/458 [17:05<12:02,  3.47s/it]
                                                 

 55%|█████▍    | 250/458 [17:05<12:02,  3.47s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.84it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.13it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:27,  1.00s/it][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.07s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.08s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.09s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.09s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.09s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A
                                                 

                                               
[A
 55%|█████▍    | 250/458 [17:41<12:02,  3.47s/it]

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A

                                               [A
 55%|█████▍    | 251/458 [17:45<49:30, 14.35s/it]
 55%|█████▌    | 252/458 [17:48<38:05, 11.09s/it]
 55%|█████▌    | 253/458 [17:52<30:00,  8.78s/it]
 55%|█████▌    | 254/458 [17:55<24:28,  7.20s/it]
 56%|█████▌    | 255/458 [17:59<20:34,  6.08s/it]
 56%|█████▌    | 256/458 [18:02<17:51,  5.31s/it]
 56%|█████▌    | 257/458 [18:06<16:02,  4.79s/it]
 56%|█████▋    | 258/458 [18:09<14:32,  4.36s/it]
 57%|█████▋    | 259/458 [18:12<13:35,  4.10s/it]
 57%|█████▋    | 260/458 [18:16<12:54,  3.91s/it]
                                                 

 57%|█████▋    | 260/458 [18:16<12:54,  3.91s/it]
 57%|█████▋    | 261/458 [18:19<12:27,  3.79s/it]
 57%|█████▋    | 262/458 [18:23<12:03,  3.69s/it]
 57%|█████▋    | 263/458 [18:26<11:48,  3.63s/it]
 58%|█████▊    | 264/458 [18:30<11:36,  3.59s/it]
 58%|█████▊    | 265/458 [18:33<11:26,  3.56s/it]
 58%|█████▊    | 266/458 [18:37<11:16,  3.53s/it]
 58%|█████▊    | 267/458 [18:40<11:08,  3.50s/it]
 59%|█████▊    | 268/458 [18:44<11:01,  3.48s/it]
 59%|█████▊    | 269/458 [18:47<10:58,  3.48s/it]
 59%|█████▉    | 270/458 [18:51<10:55,  3.48s/it]
                                                 

 59%|█████▉    | 270/458 [18:51<10:55,  3.48s/it]
 59%|█████▉    | 271/458 [18:54<10:46,  3.46s/it]
 59%|█████▉    | 272/458 [18:57<10:42,  3.45s/it]
 60%|█████▉    | 273/458 [19:01<10:38,  3.45s/it]
 60%|█████▉    | 274/458 [19:04<10:37,  3.46s/it]
 60%|██████    | 275/458 [19:08<10:34,  3.47s/it]
 60%|██████    | 276/458 [19:11<10:26,  3.44s/it]
 60%|██████    | 277/458 [19:15<10:26,  3.46s/it]
 61%|██████    | 278/458 [19:18<10:18,  3.44s/it]
 61%|██████    | 279/458 [19:22<10:12,  3.42s/it]
 61%|██████    | 280/458 [19:25<10:09,  3.42s/it]
                                                 

 61%|██████    | 280/458 [19:25<10:09,  3.42s/it]
 61%|██████▏   | 281/458 [19:28<10:04,  3.41s/it]
 62%|██████▏   | 282/458 [19:32<10:01,  3.42s/it]
 62%|██████▏   | 283/458 [19:35<10:01,  3.44s/it]
 62%|██████▏   | 284/458 [19:39<10:00,  3.45s/it]
 62%|██████▏   | 285/458 [19:42<09:53,  3.43s/it]
 62%|██████▏   | 286/458 [19:46<09:52,  3.45s/it]
 63%|██████▎   | 287/458 [19:49<09:51,  3.46s/it]
 63%|██████▎   | 288/458 [19:53<09:49,  3.47s/it]
 63%|██████▎   | 289/458 [19:56<09:46,  3.47s/it]
 63%|██████▎   | 290/458 [20:00<09:43,  3.48s/it]
                                                 

 63%|██████▎   | 290/458 [20:00<09:43,  3.48s/it]
 64%|██████▎   | 291/458 [20:03<09:39,  3.47s/it]
 64%|██████▍   | 292/458 [20:07<09:36,  3.47s/it]
 64%|██████▍   | 293/458 [20:10<09:34,  3.48s/it]
 64%|██████▍   | 294/458 [20:14<09:32,  3.49s/it]
 64%|██████▍   | 295/458 [20:17<09:29,  3.49s/it]
 65%|██████▍   | 296/458 [20:21<09:26,  3.50s/it]
 65%|██████▍   | 297/458 [20:24<09:21,  3.48s/it]
 65%|██████▌   | 298/458 [20:27<09:18,  3.49s/it]
 65%|██████▌   | 299/458 [20:31<09:15,  3.49s/it]
 66%|██████▌   | 300/458 [20:34<09:07,  3.47s/it]
                                                 

 66%|██████▌   | 300/458 [20:34<09:07,  3.47s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.82it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.29it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.12it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:27,  1.00s/it][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.08s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.09s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.08s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.08s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.08s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.08s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A
                                                 

                                               
[A
 66%|██████▌   | 300/458 [21:11<09:07,  3.47s/it]

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A

                                               [A
 66%|██████▌   | 301/458 [21:14<37:28, 14.32s/it]
 66%|██████▌   | 302/458 [21:17<28:44, 11.05s/it]
 66%|██████▌   | 303/458 [21:21<22:41,  8.78s/it]
 66%|██████▋   | 304/458 [21:24<18:25,  7.18s/it]
 67%|██████▋   | 305/458 [21:28<15:29,  6.07s/it]
 67%|██████▋   | 306/458 [21:31<13:25,  5.30s/it]
 67%|██████▋   | 307/458 [21:35<11:56,  4.74s/it]
 67%|██████▋   | 308/458 [21:38<10:55,  4.37s/it]
 67%|██████▋   | 309/458 [21:42<10:12,  4.11s/it]
 68%|██████▊   | 310/458 [21:45<09:40,  3.92s/it]
                                                 

 68%|██████▊   | 310/458 [21:45<09:40,  3.92s/it]
 68%|██████▊   | 311/458 [21:49<09:17,  3.79s/it]
 68%|██████▊   | 312/458 [21:52<09:00,  3.70s/it]
 68%|██████▊   | 313/458 [21:56<08:47,  3.64s/it]
 69%|██████▊   | 314/458 [21:59<08:35,  3.58s/it]
 69%|██████▉   | 315/458 [22:03<08:27,  3.55s/it]
 69%|██████▉   | 316/458 [22:06<08:21,  3.53s/it]
 69%|██████▉   | 317/458 [22:10<08:15,  3.52s/it]
 69%|██████▉   | 318/458 [22:13<08:09,  3.49s/it]
 70%|██████▉   | 319/458 [22:16<08:00,  3.46s/it]
 70%|██████▉   | 320/458 [22:20<07:57,  3.46s/it]
                                                 

 70%|██████▉   | 320/458 [22:20<07:57,  3.46s/it]
 70%|███████   | 321/458 [22:23<07:50,  3.44s/it]
 70%|███████   | 322/458 [22:27<07:49,  3.45s/it]
 71%|███████   | 323/458 [22:30<07:47,  3.46s/it]
 71%|███████   | 324/458 [22:34<07:45,  3.47s/it]
 71%|███████   | 325/458 [22:37<07:42,  3.48s/it]
 71%|███████   | 326/458 [22:41<07:39,  3.48s/it]
 71%|███████▏  | 327/458 [22:44<07:36,  3.49s/it]
 72%|███████▏  | 328/458 [22:48<07:33,  3.49s/it]
 72%|███████▏  | 329/458 [22:51<07:27,  3.47s/it]
 72%|███████▏  | 330/458 [22:55<07:21,  3.45s/it]
                                                 

 72%|███████▏  | 330/458 [22:55<07:21,  3.45s/it]
 72%|███████▏  | 331/458 [22:58<07:19,  3.46s/it]
 72%|███████▏  | 332/458 [23:02<07:17,  3.47s/it]
 73%|███████▎  | 333/458 [23:05<07:14,  3.48s/it]
 73%|███████▎  | 334/458 [23:09<07:09,  3.47s/it]
 73%|███████▎  | 335/458 [23:12<07:07,  3.47s/it]
 73%|███████▎  | 336/458 [23:16<07:04,  3.48s/it]
 74%|███████▎  | 337/458 [23:19<06:57,  3.45s/it]
 74%|███████▍  | 338/458 [23:22<06:55,  3.46s/it]
 74%|███████▍  | 339/458 [23:26<06:52,  3.47s/it]
 74%|███████▍  | 340/458 [23:29<06:49,  3.47s/it]
                                                 

 74%|███████▍  | 340/458 [23:29<06:49,  3.47s/it]
 74%|███████▍  | 341/458 [23:33<06:47,  3.48s/it]
 75%|███████▍  | 342/458 [23:36<06:44,  3.48s/it]
 75%|███████▍  | 343/458 [23:40<06:40,  3.48s/it]
 75%|███████▌  | 344/458 [23:43<06:33,  3.45s/it]
 75%|███████▌  | 345/458 [23:47<06:30,  3.45s/it]
 76%|███████▌  | 346/458 [23:50<06:27,  3.46s/it]
 76%|███████▌  | 347/458 [23:54<06:24,  3.46s/it]
 76%|███████▌  | 348/458 [23:57<06:21,  3.47s/it]
 76%|███████▌  | 349/458 [24:01<06:24,  3.52s/it]
 76%|███████▋  | 350/458 [24:04<06:19,  3.52s/it]
                                                 

 76%|███████▋  | 350/458 [24:04<06:19,  3.52s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.83it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.29it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.12it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:26,  1.00it/s][A

 21%|██        | 7/33 [00:06<00:26,  1.02s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.04s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.05s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.06s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.07s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.07s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.08s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.08s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.08s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.08s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.08s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.08s/it][A

 64%|██████▎   | 21/33 [00:21<00:12,  1.08s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.08s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.08s/it][A

 73%|███████▎  | 24/33 [00:24<00:09,  1.08s/it][A

 76%|███████▌  | 25/33 [00:25<00:08,  1.08s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.08s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.08s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.08s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.08s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.08s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.08s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.08s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A
                                                 

                                               
[A
 76%|███████▋  | 350/458 [24:40<06:19,  3.52s/it]

100%|██████████| 33/33 [00:34<00:00,  1.01it/s][A

                                               [A
 77%|███████▋  | 351/458 [24:44<25:32, 14.32s/it]
 77%|███████▋  | 352/458 [24:47<19:34, 11.08s/it]
 77%|███████▋  | 353/458 [24:51<15:24,  8.81s/it]
 77%|███████▋  | 354/458 [24:54<12:30,  7.21s/it]
 78%|███████▊  | 355/458 [24:58<10:28,  6.10s/it]
 78%|███████▊  | 356/458 [25:01<09:01,  5.31s/it]
 78%|███████▊  | 357/458 [25:05<08:01,  4.77s/it]
 78%|███████▊  | 358/458 [25:08<07:18,  4.39s/it]
 78%|███████▊  | 359/458 [25:12<06:48,  4.12s/it]
 79%|███████▊  | 360/458 [25:15<06:25,  3.94s/it]
                                                 

 79%|███████▊  | 360/458 [25:15<06:25,  3.94s/it]
 79%|███████▉  | 361/458 [25:19<06:09,  3.81s/it]
 79%|███████▉  | 362/458 [25:22<05:57,  3.72s/it]
 79%|███████▉  | 363/458 [25:26<05:44,  3.62s/it]
 79%|███████▉  | 364/458 [25:29<05:34,  3.55s/it]
 80%|███████▉  | 365/458 [25:33<05:27,  3.52s/it]
 80%|███████▉  | 366/458 [25:36<05:21,  3.50s/it]
 80%|████████  | 367/458 [25:39<05:15,  3.46s/it]
 80%|████████  | 368/458 [25:43<05:12,  3.47s/it]
 81%|████████  | 369/458 [25:46<05:09,  3.47s/it]
 81%|████████  | 370/458 [25:50<05:05,  3.48s/it]
                                                 

 81%|████████  | 370/458 [25:50<05:05,  3.48s/it]
 81%|████████  | 371/458 [25:53<05:02,  3.48s/it]
 81%|████████  | 372/458 [25:57<04:57,  3.46s/it]
 81%|████████▏ | 373/458 [26:00<04:55,  3.47s/it]
 82%|████████▏ | 374/458 [26:04<04:52,  3.48s/it]
 82%|████████▏ | 375/458 [26:07<04:46,  3.45s/it]
 82%|████████▏ | 376/458 [26:11<04:43,  3.46s/it]
 82%|████████▏ | 377/458 [26:14<04:38,  3.44s/it]
 83%|████████▎ | 378/458 [26:17<04:34,  3.43s/it]
 83%|████████▎ | 379/458 [26:21<04:29,  3.42s/it]
 83%|████████▎ | 380/458 [26:24<04:28,  3.44s/it]
                                                 

 83%|████████▎ | 380/458 [26:24<04:28,  3.44s/it]
 83%|████████▎ | 381/458 [26:28<04:23,  3.43s/it]
 83%|████████▎ | 382/458 [26:31<04:22,  3.45s/it]
 84%|████████▎ | 383/458 [26:35<04:19,  3.46s/it]
 84%|████████▍ | 384/458 [26:38<04:14,  3.44s/it]
 84%|████████▍ | 385/458 [26:42<04:12,  3.46s/it]
 84%|████████▍ | 386/458 [26:45<04:09,  3.46s/it]
 84%|████████▍ | 387/458 [26:48<04:06,  3.47s/it]
 85%|████████▍ | 388/458 [26:52<04:02,  3.47s/it]
 85%|████████▍ | 389/458 [26:55<03:58,  3.46s/it]
 85%|████████▌ | 390/458 [26:59<03:54,  3.45s/it]
                                                 

 85%|████████▌ | 390/458 [26:59<03:54,  3.45s/it]
 85%|████████▌ | 391/458 [27:02<03:49,  3.43s/it]
 86%|████████▌ | 392/458 [27:06<03:47,  3.44s/it]
 86%|████████▌ | 393/458 [27:09<03:42,  3.42s/it]
 86%|████████▌ | 394/458 [27:12<03:39,  3.43s/it]
 86%|████████▌ | 395/458 [27:16<03:36,  3.43s/it]
 86%|████████▋ | 396/458 [27:19<03:33,  3.45s/it]
 87%|████████▋ | 397/458 [27:23<03:30,  3.46s/it]
 87%|████████▋ | 398/458 [27:26<03:27,  3.47s/it]
 87%|████████▋ | 399/458 [27:30<03:24,  3.47s/it]
 87%|████████▋ | 400/458 [27:33<03:20,  3.45s/it]
                                                 

 87%|████████▋ | 400/458 [27:33<03:20,  3.45s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.84it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.13it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.05it/s][A

 18%|█▊        | 6/33 [00:05<00:26,  1.00it/s][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.08s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.09s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.08s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.09s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.08s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:11,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.09s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.09s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.04it/s][A
                                                 

                                               
[A
 87%|████████▋ | 400/458 [28:09<03:20,  3.45s/it]

100%|██████████| 33/33 [00:34<00:00,  1.04it/s][A

                                               [A
 88%|████████▊ | 401/458 [28:13<13:35, 14.30s/it]
 88%|████████▊ | 402/458 [28:16<10:18, 11.05s/it]
 88%|████████▊ | 403/458 [28:20<08:01,  8.75s/it]
 88%|████████▊ | 404/458 [28:23<06:26,  7.16s/it]
 88%|████████▊ | 405/458 [28:27<05:20,  6.06s/it]
 89%|████████▊ | 406/458 [28:30<04:34,  5.28s/it]
 89%|████████▉ | 407/458 [28:34<04:02,  4.75s/it]
 89%|████████▉ | 408/458 [28:37<03:36,  4.34s/it]
 89%|████████▉ | 409/458 [28:40<03:19,  4.07s/it]
 90%|████████▉ | 410/458 [28:44<03:06,  3.90s/it]
                                                 

 90%|████████▉ | 410/458 [28:44<03:06,  3.90s/it]
 90%|████████▉ | 411/458 [28:47<02:56,  3.76s/it]
 90%|████████▉ | 412/458 [28:51<02:48,  3.67s/it]
 90%|█████████ | 413/458 [28:54<02:42,  3.61s/it]
 90%|█████████ | 414/458 [28:58<02:37,  3.58s/it]
 91%|█████████ | 415/458 [29:01<02:32,  3.55s/it]
 91%|█████████ | 416/458 [29:05<02:27,  3.52s/it]
 91%|█████████ | 417/458 [29:08<02:23,  3.51s/it]
 91%|█████████▏| 418/458 [29:12<02:20,  3.51s/it]
 91%|█████████▏| 419/458 [29:15<02:16,  3.51s/it]
 92%|█████████▏| 420/458 [29:19<02:13,  3.50s/it]
                                                 

 92%|█████████▏| 420/458 [29:19<02:13,  3.50s/it]
 92%|█████████▏| 421/458 [29:22<02:09,  3.50s/it]
 92%|█████████▏| 422/458 [29:26<02:06,  3.50s/it]
 92%|█████████▏| 423/458 [29:29<02:02,  3.50s/it]
 93%|█████████▎| 424/458 [29:33<01:58,  3.49s/it]
 93%|█████████▎| 425/458 [29:36<01:54,  3.46s/it]
 93%|█████████▎| 426/458 [29:40<01:50,  3.47s/it]
 93%|█████████▎| 427/458 [29:43<01:47,  3.47s/it]
 93%|█████████▎| 428/458 [29:47<01:44,  3.48s/it]
 94%|█████████▎| 429/458 [29:50<01:40,  3.45s/it]
 94%|█████████▍| 430/458 [29:53<01:36,  3.45s/it]
                                                 

 94%|█████████▍| 430/458 [29:53<01:36,  3.45s/it]
 94%|█████████▍| 431/458 [29:57<01:33,  3.47s/it]
 94%|█████████▍| 432/458 [30:00<01:29,  3.45s/it]
 95%|█████████▍| 433/458 [30:04<01:26,  3.45s/it]
 95%|█████████▍| 434/458 [30:07<01:23,  3.47s/it]
 95%|█████████▍| 435/458 [30:11<01:20,  3.48s/it]
 95%|█████████▌| 436/458 [30:14<01:16,  3.49s/it]
 95%|█████████▌| 437/458 [30:18<01:13,  3.48s/it]
 96%|█████████▌| 438/458 [30:21<01:09,  3.46s/it]
 96%|█████████▌| 439/458 [30:25<01:05,  3.44s/it]
 96%|█████████▌| 440/458 [30:28<01:01,  3.43s/it]
                                                 

 96%|█████████▌| 440/458 [30:28<01:01,  3.43s/it]
 96%|█████████▋| 441/458 [30:31<00:58,  3.42s/it]
 97%|█████████▋| 442/458 [30:35<00:54,  3.41s/it]
 97%|█████████▋| 443/458 [30:38<00:51,  3.44s/it]
 97%|█████████▋| 444/458 [30:42<00:48,  3.44s/it]
 97%|█████████▋| 445/458 [30:45<00:44,  3.43s/it]
 97%|█████████▋| 446/458 [30:49<00:41,  3.43s/it]
 98%|█████████▊| 447/458 [30:52<00:37,  3.45s/it]
 98%|█████████▊| 448/458 [30:55<00:34,  3.43s/it]
 98%|█████████▊| 449/458 [30:59<00:30,  3.43s/it]
 98%|█████████▊| 450/458 [31:02<00:27,  3.44s/it]
                                                 

 98%|█████████▊| 450/458 [31:02<00:27,  3.44s/it]

  0%|          | 0/33 [00:00<?, ?it/s][A

  6%|▌         | 2/33 [00:01<00:16,  1.84it/s][A

  9%|▉         | 3/33 [00:02<00:23,  1.30it/s][A

 12%|█▏        | 4/33 [00:03<00:25,  1.13it/s][A

 15%|█▌        | 5/33 [00:04<00:26,  1.04it/s][A

 18%|█▊        | 6/33 [00:05<00:27,  1.00s/it][A

 21%|██        | 7/33 [00:06<00:26,  1.03s/it][A

 24%|██▍       | 8/33 [00:07<00:26,  1.05s/it][A

 27%|██▋       | 9/33 [00:08<00:25,  1.06s/it][A

 30%|███       | 10/33 [00:09<00:24,  1.07s/it][A

 33%|███▎      | 11/33 [00:10<00:23,  1.08s/it][A

 36%|███▋      | 12/33 [00:11<00:22,  1.08s/it][A

 39%|███▉      | 13/33 [00:13<00:21,  1.08s/it][A

 42%|████▏     | 14/33 [00:14<00:20,  1.09s/it][A

 45%|████▌     | 15/33 [00:15<00:19,  1.09s/it][A

 48%|████▊     | 16/33 [00:16<00:18,  1.09s/it][A

 52%|█████▏    | 17/33 [00:17<00:17,  1.09s/it][A

 55%|█████▍    | 18/33 [00:18<00:16,  1.09s/it][A

 58%|█████▊    | 19/33 [00:19<00:15,  1.09s/it][A

 61%|██████    | 20/33 [00:20<00:14,  1.09s/it][A

 64%|██████▎   | 21/33 [00:21<00:13,  1.09s/it][A

 67%|██████▋   | 22/33 [00:22<00:12,  1.09s/it][A

 70%|██████▉   | 23/33 [00:23<00:10,  1.09s/it][A

 73%|███████▎  | 24/33 [00:25<00:09,  1.09s/it][A

 76%|███████▌  | 25/33 [00:26<00:08,  1.09s/it][A

 79%|███████▉  | 26/33 [00:27<00:07,  1.09s/it][A

 82%|████████▏ | 27/33 [00:28<00:06,  1.09s/it][A

 85%|████████▍ | 28/33 [00:29<00:05,  1.09s/it][A

 88%|████████▊ | 29/33 [00:30<00:04,  1.09s/it][A

 91%|█████████ | 30/33 [00:31<00:03,  1.09s/it][A

 94%|█████████▍| 31/33 [00:32<00:02,  1.09s/it][A

 97%|█████████▋| 32/33 [00:33<00:01,  1.09s/it][A

100%|██████████| 33/33 [00:34<00:00,  1.02it/s][A
                                                 

                                               
[A
 98%|█████████▊| 450/458 [31:39<00:27,  3.44s/it]

100%|██████████| 33/33 [00:34<00:00,  1.02it/s][A

                                               [A
 98%|█████████▊| 451/458 [31:42<01:40, 14.33s/it]
 99%|█████████▊| 452/458 [31:46<01:06, 11.08s/it]
 99%|█████████▉| 453/458 [31:49<00:44,  8.80s/it]
 99%|█████████▉| 454/458 [31:53<00:28,  7.21s/it]
 99%|█████████▉| 455/458 [31:56<00:18,  6.07s/it]
100%|█████████▉| 456/458 [31:59<00:10,  5.26s/it]
100%|█████████▉| 457/458 [32:03<00:04,  4.70s/it]
100%|██████████| 458/458 [32:04<00:00,  3.60s/it]
                                                 

100%|██████████| 458/458 [32:04<00:00,  3.60s/it]
100%|██████████| 458/458 [32:04<00:00,  4.20s/it]

SUCCESS: Deepspeed command for TRAINING on CT completed.


########################################
Sequence: binary_CT_to_Pathology | Stage 2/2: Processing Dataset: Pathology
Input model for this stage: trained_on_CT
########################################
Starting training for sequence 'binary_CT_to_Pathology', stage: CT_then_Pathology on dataset Pathology
Output directory for this stage: /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT_then_Pathology

==================== Executing TRAINING ====================
Model: Seq: binary_CT_to_Pathology, Stage Input: trained_on_CT
Dataset: Pathology
Command: deepspeed --master_port=29742 /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT --data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/Pathology/train/annotations.json --image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/Pathology/train --output_dir /vol/biomedic3/mv320/projects/VLMs/MEG_x_CL/LLaVA-Med/checkpoints/binary_sequential_finetuning_pipeline/binary_CT_to_Pathology/trained_on_CT_then_Pathology --eval_data_path /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/Pathology/val/annotations.json --eval_image_folder /vol/biomedic3/mv320/data/medical_vqa/modality_specific_datasets_balanced/Pathology/val --wandb_project llava-med-binary-sequential-training_binary_CT_to_Pathology --version mistral_instruct --tune_mm_mlp_adapter True --bf16 True --num_train_epochs 2 --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --learning_rate 1e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 10 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --save_strategy steps --save_total_limit 1 --evaluation_strategy steps --load_best_model_at_end True --metric_for_best_model eval_loss --greater_is_better False --prediction_loss_only True --eval_from_train_set False --evaluation_strategy steps --eval_steps 50 --eval_accumulation_steps 1

